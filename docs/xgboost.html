<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 5 XGBoost | Applications of Machine Learning in Imputation</title>
  <meta name="description" content="This document presents the findings from the 2018/19 project into the use of machine learning in imputation." />
  <meta name="generator" content="bookdown 0.10.1 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 5 XGBoost | Applications of Machine Learning in Imputation" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This document presents the findings from the 2018/19 project into the use of machine learning in imputation." />
  <meta name="github-repo" content="Vinayak-NZ/ML-Imputation" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 5 XGBoost | Applications of Machine Learning in Imputation" />
  
  <meta name="twitter:description" content="This document presents the findings from the 2018/19 project into the use of machine learning in imputation." />
  

<meta name="author" content="Methodology" />


<meta name="date" content="2019-01-01" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="why-use-machine-learning.html">
<link rel="next" href="methods-1.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><img src="images/ONS.png"></li>
<li><a href="https://www.ons.gov.uk/methodology/methodologytopicsandstatisticalconcepts">Office for National Statistics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="what-is-imputation.html"><a href="what-is-imputation.html"><i class="fa fa-check"></i><b>2</b> What is Imputation?</a><ul>
<li class="chapter" data-level="2.1" data-path="what-is-imputation.html"><a href="what-is-imputation.html#why-is-imputation-carried-out"><i class="fa fa-check"></i><b>2.1</b> Why is imputation carried out?</a></li>
<li class="chapter" data-level="2.2" data-path="what-is-imputation.html"><a href="what-is-imputation.html#methods"><i class="fa fa-check"></i><b>2.2</b> Methods</a><ul>
<li class="chapter" data-level="2.2.1" data-path="what-is-imputation.html"><a href="what-is-imputation.html#interactive-treatment"><i class="fa fa-check"></i><b>2.2.1</b> Interactive treatment</a></li>
<li class="chapter" data-level="2.2.2" data-path="what-is-imputation.html"><a href="what-is-imputation.html#deductive-imputation"><i class="fa fa-check"></i><b>2.2.2</b> Deductive imputation</a></li>
<li class="chapter" data-level="2.2.3" data-path="what-is-imputation.html"><a href="what-is-imputation.html#model-based-imputation"><i class="fa fa-check"></i><b>2.2.3</b> Model based imputation</a></li>
<li class="chapter" data-level="2.2.4" data-path="what-is-imputation.html"><a href="what-is-imputation.html#donor-based-imputation"><i class="fa fa-check"></i><b>2.2.4</b> Donor based imputation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html"><i class="fa fa-check"></i><b>3</b> What is Machine Learning?</a><ul>
<li class="chapter" data-level="3.1" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html#supervision"><i class="fa fa-check"></i><b>3.1</b> Supervision</a><ul>
<li class="chapter" data-level="3.1.1" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html#supervised-learning"><i class="fa fa-check"></i><b>3.1.1</b> Supervised learning</a></li>
<li class="chapter" data-level="3.1.2" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html#unsupervised-learning"><i class="fa fa-check"></i><b>3.1.2</b> Unsupervised learning</a></li>
<li class="chapter" data-level="3.1.3" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html#semi-supervised-learning"><i class="fa fa-check"></i><b>3.1.3</b> Semi-supervised learning</a></li>
<li class="chapter" data-level="3.1.4" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html#reinforcement-learning"><i class="fa fa-check"></i><b>3.1.4</b> Reinforcement learning</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html#batch-and-online-learning"><i class="fa fa-check"></i><b>3.2</b> Batch and Online learning</a><ul>
<li class="chapter" data-level="3.2.1" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html#batch-learning"><i class="fa fa-check"></i><b>3.2.1</b> Batch learning</a></li>
<li class="chapter" data-level="3.2.2" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html#online-learning"><i class="fa fa-check"></i><b>3.2.2</b> Online learning</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html#approaches-to-generalisation"><i class="fa fa-check"></i><b>3.3</b> Approaches to generalisation</a><ul>
<li class="chapter" data-level="3.3.1" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html#instance-based-learning"><i class="fa fa-check"></i><b>3.3.1</b> Instance-based learning</a></li>
<li class="chapter" data-level="3.3.2" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html#model-based-learning"><i class="fa fa-check"></i><b>3.3.2</b> Model-based learning</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="why-use-machine-learning.html"><a href="why-use-machine-learning.html"><i class="fa fa-check"></i><b>4</b> Why use Machine Learning?</a></li>
<li class="chapter" data-level="5" data-path="xgboost.html"><a href="xgboost.html"><i class="fa fa-check"></i><b>5</b> XGBoost</a><ul>
<li class="chapter" data-level="5.1" data-path="xgboost.html"><a href="xgboost.html#decision-trees"><i class="fa fa-check"></i><b>5.1</b> Decision trees</a><ul>
<li class="chapter" data-level="5.1.1" data-path="xgboost.html"><a href="xgboost.html#boosting"><i class="fa fa-check"></i><b>5.1.1</b> Boosting</a></li>
<li class="chapter" data-level="5.1.2" data-path="xgboost.html"><a href="xgboost.html#building-models-with-xgboost"><i class="fa fa-check"></i><b>5.1.2</b> Building models with XGBoost</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="methods-1.html"><a href="methods-1.html"><i class="fa fa-check"></i><b>6</b> Methods</a><ul>
<li class="chapter" data-level="6.1" data-path="methods-1.html"><a href="methods-1.html#census-teaching-file"><i class="fa fa-check"></i><b>6.1</b> Census Teaching File</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="results.html"><a href="results.html"><i class="fa fa-check"></i><b>7</b> Results</a><ul>
<li class="chapter" data-level="7.1" data-path="results.html"><a href="results.html#summary-statistics"><i class="fa fa-check"></i><b>7.1</b> Summary statistics</a></li>
<li class="chapter" data-level="7.2" data-path="results.html"><a href="results.html#comparison-of-imputation-methods"><i class="fa fa-check"></i><b>7.2</b> Comparison of imputation methods</a><ul>
<li class="chapter" data-level="7.2.1" data-path="results.html"><a href="results.html#economic-activity"><i class="fa fa-check"></i><b>7.2.1</b> Economic Activity</a></li>
<li class="chapter" data-level="7.2.2" data-path="results.html"><a href="results.html#hours-worked"><i class="fa fa-check"></i><b>7.2.2</b> Hours worked</a></li>
<li class="chapter" data-level="7.2.3" data-path="results.html"><a href="results.html#social-grade"><i class="fa fa-check"></i><b>7.2.3</b> Social Grade</a></li>
<li class="chapter" data-level="7.2.4" data-path="results.html"><a href="results.html#student"><i class="fa fa-check"></i><b>7.2.4</b> Student</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="next-steps.html"><a href="next-steps.html"><i class="fa fa-check"></i><b>8</b> Next steps</a></li>
<li class="chapter" data-level="9" data-path="resources.html"><a href="resources.html"><i class="fa fa-check"></i><b>9</b> Resources</a><ul>
<li class="chapter" data-level="9.1" data-path="resources.html"><a href="resources.html#r-scripts"><i class="fa fa-check"></i><b>9.1</b> R scripts</a></li>
<li class="chapter" data-level="9.2" data-path="resources.html"><a href="resources.html#data"><i class="fa fa-check"></i><b>9.2</b> Data</a></li>
<li class="chapter" data-level="9.3" data-path="resources.html"><a href="resources.html#xgboost-1"><i class="fa fa-check"></i><b>9.3</b> XGBoost</a></li>
<li class="chapter" data-level="9.4" data-path="resources.html"><a href="resources.html#donor-imputation"><i class="fa fa-check"></i><b>9.4</b> Donor imputation</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Applications of Machine Learning in Imputation</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="xgboost" class="section level1">
<h1><span class="header-section-number">Chapter 5</span> XGBoost</h1>
<p>XGBoost is a set of open source functions and steps, referred to as a library, that use supervised ML where analysts specify an outcome to be estimated/ predicted. The XGBoost library uses multiple decision trees to predict an outcome.</p>
<p>The ML system is trained using batch learning and generalised through a model based approach. It uses all available data to construct a model that specifies the relationship between the predictor and outcome variables, which are then generalised to the test dataset.</p>
<p>XGBoost stands for eXtreme Gradient Boosting. The word “extreme” reflects its goal to push the limit of computational resources. Whereas gradient boosting is a machine learning technique for regression and classification problems that optimises a collection of weak prediction models in an attempt to build an accurate and reliable predictor.</p>
<p>In order to build a better understanding of how XGBoost works, the documentation will briefly review:</p>
<ul>
<li>Decision trees: How decision trees play a role in XGBoost?<br />
</li>
<li>Boosting: What is it?</li>
</ul>
<p>The final section of this chapter provides a step by step guide on building models using XGBoost; the reader is encouraged to use this code to predict an outcome variable using available auxiliary variables.</p>
<div id="decision-trees" class="section level2">
<h2><span class="header-section-number">5.1</span> Decision trees</h2>
<p>Decision trees can be used as a method for grouping units in a dataset by asking questions, such as “Does an individual have a Bachelor’s degree?”. In this example, two groups would be created; one for those with a Bachelor’s degree, and one for those without. Figure 2 provides a visual depiction of this grouping in an attempt to explain Income.</p>
<div class="figure">
<img src="images/dt_bach.png" alt="Figure 2. Decision tree that splits units in a dataset based on whether individual has a Bachelor’s degree or not, in order to predict Income. The tree shows that those with a Bachelors degree (&gt; 0) on average earn more than than those wihtout a Bachelor’s degree (&lt; 0)." />
<p class="caption">Figure 2. Decision tree that splits units in a dataset based on whether individual has a Bachelor’s degree or not, in order to predict Income. The tree shows that those with a Bachelors degree (&gt; 0) on average earn more than than those wihtout a Bachelor’s degree (&lt; 0).</p>
</div>
<p>Each subsequent question in a decision tree will produce a smaller group of units. This grouping is carried out to identify units with similar characteristics with respect to an outcome variable. The model in Figure 3 attempts to use University qualifications to predict Income.</p>
<div class="figure">
<img src="images/dt_all.png" alt="Figure 3. Decision tree that splits units in a dataset based on whether individual has a Bachelor’s degree (yes/no), a Master’s degree and pHD (yes/no), in order to predict Income. The tree shows that those with a higher qualification tend to earn more." />
<p class="caption">Figure 3. Decision tree that splits units in a dataset based on whether individual has a Bachelor’s degree (yes/no), a Master’s degree and pHD (yes/no), in order to predict Income. The tree shows that those with a higher qualification tend to earn more.</p>
</div>
<p>The following characteristics are true of decision trees:</p>
<ul>
<li>A single question is asked at each decision node, and there are only two possible choices. With the example in Figure 3, the questions include 1) Does individual have a pHD, 2) Does individual have a Master’s and 3) Does individual have a Bachelor’s degree.<br />
</li>
<li>At the bottom of every decision tree, there is a single possible decision. Every possible decision will eventually lead to a choice. Some decisions will lead to a choice sooner. The model in Figure 3 attempts to predict Income using University Qualifications. Each node presents a question about whether an individual possesses a given qualification. The end nodes present the distribution of income for individuals with the specified qualifications. As a result, the choices would be the expected value of Income for an individual, given the qualificaitons obtained.</li>
</ul>
<p>Decision trees are a learning method that involve a tree like graph to model either continuous or categorical choice given some data. It is composed of a series of binary questions, which when answered in succession yield a prediciton about data at hand. XGBoost uses Classification and Regression Trees (CART), which are presented in the above examples, to predict the outcome variable.</p>
<div id="boosting" class="section level3">
<h3><span class="header-section-number">5.1.1</span> Boosting</h3>
<p>A single decision tree is considered a weak/ base learner as it only slightly better than chance at predicting the outcome variable. Whereas strong learners are any algorithm that can be tuned to achieve peak performance for supervised learning. XGBoost uses decision trees as base learners; combining many weak learners to form a strong learner. As a result it is referred to as an ensemble learning method; using the output of many models (i.e. trees) in the final prediction.</p>
<p>The concept of combining many weak learners to produce a strong learner is referred as boosting. XGBoost will iteratively build a set of weak models on subsets of the data; weighting each weak prediction according to the weak learner’s performance. A prediction is derived by taking the weighted sum of all base learners.</p>
</div>
<div id="building-models-with-xgboost" class="section level3">
<h3><span class="header-section-number">5.1.2</span> Building models with XGBoost</h3>
<p>In the training data, a target variable <span class="math inline">\(y_{i}\)</span> is specified, whilst all other features <span class="math inline">\(x_{i}\)</span> are used as predictors of the target variable. A collection of decision trees are used to predict values of <span class="math inline">\(y_{i}\)</span> using <span class="math inline">\(x_{i}\)</span>. Individually, each decision tree, would be a weak predictor of the outcome variable. However, as a collective, the decision trees may enable analysts to make accurate and reliable predictions of <span class="math inline">\(y_{i}\)</span>. As a result, the method for predicting the target variable using <span class="math inline">\(x_{i}\)</span> in XGBoost is referred to as decision tree ensembles. The steps below demonstrate how XGBoost was used to build a model, to predict income, using Univeristy Qualifications.</p>
<ol style="list-style-type: decimal">
<li>Load the following packages</li>
</ol>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(caret)

<span class="kw">library</span>(xgboost)</code></pre></div>
<ol start="2" style="list-style-type: decimal">
<li>Load the dataset and remove the identifer</li>
</ol>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Load data</span>
<span class="kw">load</span>(<span class="st">&quot;data/Income_tree.RData&quot;</span>)

<span class="co"># Remove identifier</span>
Income &lt;-<span class="st"> </span>Income[,<span class="op">-</span><span class="dv">1</span>]</code></pre></div>
<ol start="3" style="list-style-type: decimal">
<li>Split the dataset into training and test</li>
</ol>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Split data into training and test</span>
<span class="kw">set.seed</span>(<span class="dv">5</span>)

s &lt;-<span class="st"> </span><span class="kw">createDataPartition</span>(Income<span class="op">$</span>income, <span class="dt">p =</span> <span class="fl">0.8</span>, <span class="dt">list=</span><span class="ot">FALSE</span>)

training &lt;-<span class="st"> </span>Income[s,]

test &lt;-<span class="st"> </span>Income[<span class="op">-</span>s,]</code></pre></div>
<ol start="4" style="list-style-type: decimal">
<li>Convert the data into DMatrix objects, which is the recommended input type for xgboost</li>
</ol>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Convert the data to matrix and assign output variable</span>
train.outcome &lt;-<span class="st"> </span>training<span class="op">$</span>income

train.predictors &lt;-<span class="st"> </span><span class="kw">sparse.model.matrix</span>(income <span class="op">~</span><span class="st"> </span>.,
                                        <span class="dt">data =</span> training
)[, <span class="op">-</span><span class="dv">1</span>]

test.outcome &lt;-<span class="st"> </span>test<span class="op">$</span>income

test.predictors &lt;-<span class="st"> </span><span class="kw">model.matrix</span>(income <span class="op">~</span><span class="st"> </span>.,
                                <span class="dt">data =</span> test
)[, <span class="op">-</span><span class="dv">1</span>]

<span class="co"># Convert the matrix objects to DMatrix objects</span>
dtrain &lt;-<span class="st"> </span><span class="kw">xgb.DMatrix</span>(train.predictors, <span class="dt">label=</span>train.outcome)

dtest &lt;-<span class="st"> </span><span class="kw">xgb.DMatrix</span>(test.predictors)</code></pre></div>
<ol start="5" style="list-style-type: decimal">
<li>Train the model</li>
</ol>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Train the model</span>
model &lt;-<span class="st"> </span><span class="kw">xgboost</span>(
  <span class="dt">data =</span> dtrain, <span class="dt">max_depth =</span> <span class="dv">2</span>, <span class="dt">eta =</span> <span class="dv">1</span>, <span class="dt">nthread =</span> <span class="dv">2</span>, <span class="dt">nrounds =</span> <span class="dv">10</span>,
  <span class="dt">objective =</span> <span class="st">&quot;reg:linear&quot;</span>)</code></pre></div>
<ol start="6" style="list-style-type: decimal">
<li>Test the model</li>
</ol>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Test the model</span>
pred &lt;-<span class="st"> </span><span class="kw">predict</span>(model, dtest)

<span class="co"># Evaluate the performance of model</span>
<span class="kw">RMSE</span>(pred,test.outcome)</code></pre></div>
<ol start="7" style="list-style-type: decimal">
<li>Examine the importance of each feature in the model</li>
</ol>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Examine feature importance</span>
importance_matrix &lt;-<span class="st"> </span><span class="kw">xgb.importance</span>(<span class="dt">model =</span> model)

<span class="kw">print</span>(importance_matrix)

<span class="kw">xgb.plot.importance</span>(<span class="dt">importance_matrix =</span> importance_matrix)</code></pre></div>
<ol start="8" style="list-style-type: decimal">
<li>Plot the individual trees in the model</li>
</ol>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Plot the trees</span>
<span class="co"># Tree 1</span>
<span class="kw">xgb.plot.tree</span>(<span class="dt">model =</span> model, <span class="dt">tree=</span><span class="dv">0</span>)
<span class="co"># Tree 2</span>
<span class="kw">xgb.plot.tree</span>(<span class="dt">model =</span> model, <span class="dt">tree=</span><span class="dv">1</span>)
<span class="co"># Tree 3</span>
<span class="kw">xgb.plot.tree</span>(<span class="dt">model =</span> model, <span class="dt">tree=</span><span class="dv">2</span>)</code></pre></div>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="why-use-machine-learning.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="methods-1.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/04-how_XGBoost.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"download": ["Machine-Learning-in-Imputation.pdf", "Machine-Learning-in-Imputation.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
